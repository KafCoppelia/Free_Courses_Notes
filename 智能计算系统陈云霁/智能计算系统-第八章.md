# Chapter 8 智能编程语言
Revision: 2

## 8.1 为什么需要智能编程语言？

![Why_Need_Intelligent_Programming_Languages](./Images/8.1-1-Why_Need_Intelligent_Programming_Languages.png)

### 语义鸿沟

以深度学习中最为核心的卷积运算为例：

- C++：标量计算；
- Python：向量语义(Array)；
- 有Conv语义和Tensor类型的编程语言；

![Gap_of_Languages](./Images/8.1-2-Gap_of_Languages.png)

![Gap_of_Languages2](./Images/8.1-3-Gap_of_Languages2.png)

### 硬件鸿沟

![Gap_of_Hardwares](./Images/8.1-4-Gap_of_Hardwares.png)

- 智能计算硬件在**控制**、**存储**、**计算**等方面有独特性；
- 传统编程语言难以有效描述上述硬件特点；
- 不同层次编程语言和硬件特性带来的性能影响；
- **存储逻辑**上一般采用程序员可见的Scratchpad Memory(SPM)，而不是通用平台上程序员透明的Cache；
- **计算逻辑**上提供了面向智能计算的定制运算单元，如16位浮点、Brain浮点等，其精度损失几乎可忽略；

![Gap_of_Hardwares2](./Images/8.1-5-Gap_of_Hardwares2.png)

### 平台鸿沟

- 功能可移植性：采用特定平台专用语言所编写的程序能够在别的平台上正常运行；
	- 矩阵乘法的例子调用了AVX的intrinsic函数，在ARM上无法运行；
- 性能可移植性：在特定平台上优化好的程序，在新的硬件平台上仍然保证有较高的执行效率；
- 理想的编程语言需要**抽取不同硬件平台的共性特征**，并在此基础上提取**性能关键特征作为语言特性**提供给用户；

### 小结

- 面向**语义**、**硬件**、**平台**三大鸿沟，传统编程语言难以满足需求；
- 领域专用编程语言是满足智能计算**高开发效率**、**高性能**和**高可移植性**的重要途径；

![Comparison_of_Languages](./Images/8.1-6-Comparison_of_Languages.png)

## 8.2 智能计算系统抽象架构

### 抽象硬件架构

![Hierarchical_Abstract_Hardware_Architecture](./Images/8.2-1-Hierarchical_Abstract_Hardware_Architecture.png)

- 层次化的智能计算系统抽象硬件架构
	- 智能计算系统中每一层都包含存储单元、控制单元和若干计算单元；
	- 每个计算单元进一步分解为子存储单元、控制单元和若干计算单元，整个系统以递归方式构成；
	- 底层的每个叶节点都是具体的加速器，用于完成最基本的计算任务；

#### 典型智能计算系统

![Typical_One](./Images/8.2-2-Typical_One.png)

- 多卡DLP服务器抽象为五个层次：
	- 服务器级(Server)；
	- 板卡级( Card)；
	- 芯片级(Chip)；
	- 处理器簇级(Cluster)；
	- 处理器核级(Core)；
	- 可以方便地通过增加各层次的规模来提升整个系统算力；

### 控制模型

- 指令是实现对计算和存储进行控制的关键。为了设计高效的指令集，需要充分分析智能领域的典型计算模型，提炼有代表性的操作，进行针对性设计。
- **对智能算法进行抽象**：
	- 控制；
	- 数据传输；
	- 计算：标量、向量、矩阵运算等；
	- 逻辑操作：标量、向量运算等；
- **关注计算与存储的交互**：尽可能将计算与存储并行。例如，将控制计算和访存的指令分开在不同的队列中发射执行，以提高并行度；

### 计算模型

- 程序员可见的护腰包括**定制运算单元**和**并行计算架构**；
- 定制运算单元
	- 智能应用具有一定误差容忍度。通过该特性，一般在智能计算系统中采用定制的低位宽运算单元（FP16、INT8、BF16，甚至INT4等）以提升处理能效；
	- 由于智能应用的多样性和复杂性，目前对于哪种低位宽最为合适无统一结论。例如，推断和训练对于精度的要求不一样，图像/视频类应用和语音类应用对于精度要求也不一样；
- 并行计算架构：任务切分和同步；

![Distribution_and_Synchronization_of_Tasks](./Images/8.2-3-Distribution_and_Synchronization_of_Tasks.png)

### 存储模型

- 智能应用中存在大量数据密集的内存访问，合理地组织存储层次和计算单元同样重要，需要两者协同设计以平衡计算与访存，实现高效的智能计算；
- 分为**全局存储**和**本地存储**；

![Model_of_Store](./Images/8.2-4-Model_of_Store.png)

## 8.3 智能编程模型

### 异构编程模型

- 异构计算系统组成
	- 通用处理器：控制设备（主机端），负责控制和调度等工作；
	- 领域处理器：从设备（设备端），负责大规模的并行计算或领域专用计算任务；
	- 二者协同完成完整计算任务；
- 典型异构计算系统
	- GPU为核心
	- FPGA为核心
	- TPU为核心
	- DLP为核心

![Heterogeneous_Programming_Model](./Images/8.3-1-Heterogeneous_Programming_Model.png)

#### 分类

从用户接口角度分为两类：

- 构建全新的异构并行编程；
- 对现有编程语言进行异构并行扩展；
- 典型异构并行编程模型对比：

|            |   类别   |         主要编程考量         |
| :--------: | :------: | :---------------------------: |
|   OpenCL   | 语言扩展 | 任务划分+数据分布、通信、同步 |
|    CUDA    | 语言扩展 | 任务划分+数据分布、通信、同步 |
| Copperhead |  新语言  |         任务划分为主         |
|   Merge   |  新语言  |         任务划分为主         |

#### 流程

编译和链接流程

- 整体采用分离式编程方式：主机端和设备端代码；
- 编译器支持是异构并行编程模型的核心；

![Process_of_Heterogeneous_Programming_Model](./Images/8.3-2-Process_of_Heterogeneous_Programming_Model.png)

![Process_of_Heterogeneous_Programming_Model2](./Images/8.3-3-Process_of_Heterogeneous_Programming_Model2.png)

#### 运行时支持

- 完成任务映射和调度，即指定任务具体在**哪个设备或计算单元**上以**何种顺序**执行；
- 分为主机端和设备端
	- 主机端：**控制部分**和**串行任务**在主机端运行；
	- 设备端：**计算部分**和**并行任务**在设备端运行；

#### 通用智能编程模型

通用智能编程模型以前述异构编程模型为基础

- Kernel定义：定义在设备端DLP上的核心计算任务；
- 编译器支持：指定DLP上的核心计算任务如何高效地翻译成目标代码
	- 任务划分：
	- 数据通信：为DLP的复杂存储层次提供支持；
	- 同步支持：为并行计算架构提供支持；
- 运行时支持：指定DLP上的核心计算任务以何种方式映射到计算单元
	- 任务调度单位；
	- 队列；

##### Kernel定义

- 与异构编程模型中的概念一致，DLP上执行的任务叫Kernel，资源允许时DLP可同时执行多个并行Kernel；
	- 每个Kernel有一个入口函数，BLC中用 `__dlp_entry__` 指定；
	- Kernel的启动需调用运行时API：`InvokeKernel` 函数；
	- 设备端程序默认的函数类型：Device函数，以 `__dlp_device__` 来修饰；

##### 编译器支持

- 任务划分
	- 并行内建变量
		- **硬件**：clusterDim（维度）、clusterId（序号）、coreDim、coreId；
		- **任务**：taskDim[XYZ]、taskId[XYZ]；
		- 表示Kernel启动task的规模，有XYZ三个维度，用户根据需求指定；
	- 任务调度类型
		- 表示Kernel运行调度时所需的硬件核；
		- BLOCK类型：Kernel为单核任务，按单核进行调度；
		- UNIONx类型：Kernel为多核并行任务（其中x可为1/2/4，UNION1对应1个cluster 4个核）；

  ![Task_Blocks](./Images/8.3-4-Task_Blocks.png)

	- 数据通信：为DLP的复杂存储层次提供支持
		- 隐式数据管理：GPR标量数据，由编译器隐式插入Load/Store指令；
		- 显式数据管理：
			- DRAM/NRAM/WRAM/SRAM间向量及张量数据；
			- 主机-DLP间DRAM数据；

  ![Communication_of_Data](./Images/8.3-5-Communication_of_Data.png)

	- 同步支持：为并行计算架构提供支持
		- `__sync_all`：**同步任务执行的所有核**，只有所有核到达同步点时才继续往下执行；
		- `__sync_cluster`：**同步一个Cluster内部的所有核**，当一个Cluster内所有核都到达同步点时才继续往下执行。与CUDA相比，GPU同一线程块内的thread可以同步，而线程块间的thread无法同步；
	- 内建运算：为用户编程提供支持，提高开发效率
		- 通用智能变成语言提供并实现了 `__conv` 与 `__mlp` 等内建函数接口，分别对应卷积和全连接等典型神经网络运算；
		- 上述接口是对C/C++的拓展，深度学习处理器端Kernel程序编写时可调用这些接口，通过编译器将这些接口翻译为底层硬件指令；
		- 通用智能编程模型直接实现了神经网络计算的内建接口，能更好地支持智能应用；

##### 运行时支持

- 任务调度单位(BLOCK/UNIONx)
	- 以调度单位将Kernel中的任务在时间或空间维度展开
		- BLOCK：单核调度，当有一个核空闲时，调度一个任务执行；
		- UNIONx：调度时需要x个cluster，当有x个cluster空闲时，调度任务执行
	- 调度单位需要用户在编程时指定。运行时只有当空闲的硬件资源数大于调度单位时，Kernel才会被调用；
- 队列(Queue)
	- 管理需要执行的任务，队列既可以单独工作，也可以协同工作；
	- 运行时（或硬件）不断把任务放到队列中，一旦硬件计算资源有空闲，就从队列中取出一个任务执行；
- 示例

![Example_of_Runtime_Support](./Images/8.3-6-Example_of_Runtime_Support.png)

![Example_of_Runtime_Support2](./Images/8.3-7-Example_of_Runtime_Support2.png)

![Example_of_Runtime_Support3](./Images/8.3-8-Example_of_Runtime_Support3.png)

### 智能编程模型实例：BANG异构编程

BANG语言是针对MLU硬件提出的编程语言，兼顾云边端等不同目标平台，提供高性能机器学习计算支持；

- 提供通用的异构编程模型，方便用户拓展自己的应用程序；
- 提供高效的编程接口，发挥底层硬件特性；
- 基于C/C++语言的拓展，简单易用；

![BangC_and_MLU](./Images/8.3-9-BangC_and_MLU.png)

#### 流程

- 采用分离式编程，即主机端与设备端程序分开编程并分别编译，最后链接成一个可执行程序；
- 主机端为C/C++程序，通常需调用**CNRT运行时接口**完成以下步骤：
	1. 准备输入数据；
	2. 拷贝输入数据到MLU；
	3. 准备Kernel参数；
	4. 创建Queue；
	5. 指定Kernel任务规模以及调度类型；
	6. 启动Kernel；
	7. MLU到主机的输出数据拷贝；
	8. 资源的释放；

![CNRT_Runtime_Interface](./Images/8.3-10-CNRT_Runtime_Interface.png)

- 设备端使用BANG语言特定的语法规则和接口进行编程；
- 编译和链接也采用分离的方式；
	- MLU端程序采用编译器CNCC进行编译，得到MLU程序的目标文件；
	- Host端程序采用普通C/C++编译器GCC/CLANG等进行编译，得到Host程序的目标文件；
	- 最后使用Host端链接器将MLU与Host程序的目标文件以及CNRT库链接成Host端的可执行文件；

![Linker_of_BANG](./Images/8.3-11-Linker_of_BANG.png)

#### 示例：主机端

![Example_of_BANGC_Programming_on_Host](./Images/8.3-12-Example_of_BANGC_Programming_on_Host.png)

#### 示例：设备端

![Example_of_BANGC_Programming_on_Device](./Images/8.3-13-Example_of_BANGC_Programming_on_Device.png)

## 8.4 智能编程语言基础

### 语法概述

- 智能编程语言考虑基于过程式语言
	- 当前大多数语言是过程式的，可以减少用户学习成本；
	- 当前主流智能算法可以描述为明确过程，适合采用过程式语言描述；
- 参考经典的过程是语言C/C++，我们所定义的智能编程语言同样具有**数据和函数**两个基本要素
	- 例如，定义 `add_func` 函数的函数体和C语言一致：

```c
int add_func(int a, int b) {
	int c = a + b;
	return c;
}
```

#### 基本数据类型

| 基本数据类型 |  长度  |                     说明                     |
| :----------: | :-----: | :-------------------------------------------: |
|    int8_t    | 1 byte |                   1字节整数                   |
|   uint8_t   | 1 byte |                1字节无符号整数                |
|   int16_t   | 2 bytes |                   2字节整数                   |
|   uint16_t   | 2 bytes |                2字节无符号整数                |
|   int32_t   | 4 bytes |                   4字节整数                   |
|   uint32_t   | 4 bytes |                4字节无符号整数                |
|     half     | 2 bytes |   半精度浮点数据类型，采用IEEE-754 fp16格式   |
|    float    | 4 bytes | IEEE-754 fp32格式浮点类型，仅支持类型转换计算 |
|     char     | 1 bytes |               对应C语言char类型               |
|     bool     | 1 bytes |               对应C语言bool类型               |
|     指针     | 8 bytes |                   指针类型                   |

#### 宏、常量与内置变量

- 宏、常量由用户定义，内置变量是语言既有的
	- 宏和常量，宏不仅可以定义常量数据，也可以定义一段代码；常量是不可修改的数据，只能在初始化时被赋值；
	- 内置变量，编程语言本身包含的常量和变量，不需用户定义即可直接使用；

| 内置变量名 |         具体含义         |
| :--------: | :----------------------: |
|   coreId   |      DLP中核的编号      |
| clusterId |      DLP中簇的编号      |
|   taskId   | 程序运行时分配的任务编号 |

#### I/O操作语句

- 不同层次的智能处理节点有各自的本地存储，需要提供不同存储层次间的数据搬移；
	- 典型的NUMA(Non-Uniform Memory ACCESS Architecture)架构：

  ![NUMA](./Images/8.4-1-NUMA.png)

	- 不同处理器核对不同位置存储器的访问速度不同。对于核0而言，其访问设备内存DDR0的速度比访问DDR1的速度更快。DDR0看作是本地存储，DDR1作为全局存储；
	- 片上存储，除了单核内NRAM和权值WRAM，还有一类共享存储，可用于簇内的多核共享；
- 针对上述典型架构（三种片上存储，两种设备内存），可以有多种不同的数据搬移操作类型：

![Movements_between_Different_RAM](./Images/8.4-2-Movements_between_Different_RAM.png)

- 针对上述搬移操作类型，可在智能编程语言中定义相应的内建函数 `__memcpy`，方便用户实现不同类型的数据搬移：

```c
void __memcpy(void* dst, void* src, uint32 bytes, Direction_t dir);
```

#### 标量计算语句

- 标量即单个数据的计算，标量计算是编程语言的基本功能
	- 智能编程语言的标量计算语句有两种形式：
		- **运算符号**（如+、-、\*、/等）；
		- **内建函数**（如abs、max、min等）；
- 智能编程语言的标量计算由编译器映射到标量计算单元，吞吐量不及张量运算，但具有良好的通用性和灵活性；

#### 张量计算语句

- 张量计算是智能编程语言的主要特点，可以通过内建函数直接映射到张量计算单元；
- 张量计算直接对精度类型和语义类型等数据直接进行操作；

|                                                       张量计算语句                                                       |   具体功能   |
| :----------------------------------------------------------------------------------------------------------------------: | :----------: |
|                               `__vec_add(float* out, float* in1, float* in2, int size)`                               |  向量对位加  |
|                               `__vec_sub(float* out, float* in1, float* in2, int size)`                               |  向量对位减  |
|                               `__vec_mul(float* out, float* in1, float* in2, int size)`                               |  向量对位乘  |
| `__conv(half* out, int8* in, int8* weight, half bias, int ci, int hi, int wi, int co, int kh, int kw, int sh, int sw)` |   卷积运算   |
|                         `__mlp(half* out, int8* in, int8* weight, half bias, int ci, int co)`                         |  全连接运算  |
|                `__maxpool(half* out, half* in, int ci, int hi, int wi, int kh, int kw, int sh, int sw)`                | 最大池化运算 |

##### 实例：BANG数学库

- BANG语言将MLU的数学类和神经网络类指令封装成库函数；
- BANG数学库函数是在MLU架构上进行高性能编程的关键；
- BANG数学库头文件：`/usr/local/neuware/lib/clang/x.x.x/include/__clang_bang_math.h`
- **使用约束**：张量计算通常是对批量数据进行操作；
	- 源和目的都是NRAM上的数据；
	- 数据的长度（字节数）必须是128的整数倍；
	- 源和目的地址必须按64位对齐；
- `__bang_add(half* dst, half* src0, half* src1, int32_t NumOfEle)`
	- 语义：两个向量相加，得到新的向量；
	- 约束：源和目标都是 `half` 类型；
- `__bang_mul_const(half* dst, half* src0, half* scale, int32_t NumOfEle)`
	- 语义：对向量的每一个元素乘以一个常数；
- `__bang_eq(half* dst, half* src0, half* src1, int32_t NumOfEle)`
	- 语义：对 `src0` 和 `src1` 两个张量进行element-wise的比较操作，并将结果存入 `dst` 中；
- `__bang_select(half* dst, half* src0, half* src1, int32_t NumOfEle)`；
- `__bang_max(half* dst, half* src, int32_t NumOfEle)`；
- `__bang_count(half* dst, half* src, int32_t NumOfEle)`；
- ……

#### 控制流语句

- 与通用编程语言一样，智能编程语言同样需要有分支和循环等控制流语句：
	- **分支语句**：用于处理程序的选择逻辑，由判断条件与分支代码段组成，与传统编程语言类似；
	- **循环语句**：用于处理程序循环逻辑，由循环执行条件和循环代码段组成，与传统编程语言类似；
	- **同步语句**：解决多核间并行数据依赖问题，保证最终计算结果正确。主要分为两类：
		- Cluster内同步(`__sync_cluster`)，保证一个Cluster内所有核同步；
		- 全局同步(`__sync_all`)，芯片内所有Cluster、所有核均同步；

#### 串行程序示例

向量中每个数求平方，每次处理64个数

![Example_of_Serial_Programming](./Images/8.4-3-Example_of_Serial_Programming.png)

#### 并行程序示例

矩阵乘法示例，在4个核上并行执行，每个核上代码一致：
- 计算1\*32和32\*32的矩阵乘法，最终得到4\*32的结果；
- 通过运行时API来指定任务规模(4\*1\*1)及调度方式(UNION1)；

![Example_of_Parallel_Programming](./Images/8.4-4-Example_of_Parallel_Programming.png)

## 8.5 智能应用编程接口

### Kernel函数接口

- 为了充分利用并行资源，需要在Kernel内部对任务进行有效切分，同时在主机端配置和调用相应的Kernel函数接口；
- 任务切分的内置变量

![Functions_of_Task_Segmentation](./Images/8.5-1-Functions_of_Task_Segmentation.png)

- 主机端Kernel函数接口
	- 用于将智能编程语言编写的程序加载到DLP上执行；主要关注Kernel参数设置和调用；
	 `GetKernelParamsBuffer(KernelParamsBuffer_t *params)`：获取Kernel的参数块结构，成功返回 `RET_SUCCESS`，否则返回相应错误码；
	 `CopyKernelParamsBuffer(KernelParamsBuffer_t dstbuf, KernelParamsBuffer_t srcbuf)`：拷贝 `srcbuf` 至 `dstbuf`，成功返回 `RET_SUCCESS`，否则返回相应错误码；
	 `KernelParamsBufferAddParam(KernelParamsBuffer_t *params, void* data, size_t bytes)`：向 `KernelParamsBuffer_t` 中增加常量参数，成功返回 `RET_SUCCESS`，否则返回相应错误码；
	- ……

### 运行时接口

- 包括**设备管理**、**队列管理**和**内存管理**等接口；
- 设备管理：主要涉及初始化、设备设置、设备销毁等操作：
	- `Init(unsigned int flags)`
	- `GetDeviceCount(unsigned int* dev_num)`
	- `GetDeviceHandle(Dev_t* pdev, int ordinal)`
	- `SetCurrentDevice(Dev_t dev)`
	- `Destroy(void)`
- 队列管理：队列是用于执行任务的环境。计算任务可下发到队列中执行。同一队列可容纳多个任务。队列有如下属性：
	- **串行性**：下发到同一队列中的任务，按下发顺序串行执行；
	- **异步性**：任务下发到队列是异步过程，即下发完成后程序控制流回到主机，主机程序继续往下执行。运行时环境提供队列的同步接口 `SyncQueue` 用于等待队列中所有任务完成；
	- **并行性**：不同队列中的任务并行执行；
- 内存管理：
	- 主机端内存管理：`hostMalloc(void** ptr, size_t bytes, ...)`、`hostFree(void* ptr)`；
	- 设备端内存管理：`devMalloc(void** ptr, size_t bytes)`、`devFree(void* ptr)`；
	- 主机与设备端内存拷贝：`Memcpy(void* dst, void* src, size_t bytes, MemTransDir_t dir)`；

### 编程接口实例：BANG程序

##### BANG单核向量加法

设备端代码实现：

![Example_of_Vector_Add_on_Single_Core_of_Device](./Images/8.5-2-Example_of_Vector_Add_on_Single_Core_of_Device.png)

主机端代码实现：

![Example_of_Vector_Add_on_Single_Core_of_Host](./Images/8.5-3-Example_of_Vector_Add_on_Single_Core_of_Host.png)

![Example_of_Vector_Add_on_Single_Core_of_Host2](./Images/8.5-4-Example_of_Vector_Add_on_Single_Core_of_Host2.png)

##### BANG多核向量加法

![Example_of_Vector_Add_on_Multi_Cores](./Images/8.5-5-Example_of_Vector_Add_on_Multi_Cores.png)

## 8.6 智能应用功能调试

### 功能调试接口

- `__assert(bool flag)`：abort the kernel if flag is false；
- `__abort()`：exit the kernel with error code -1；
- `exit(int status)`：exit the kernel with code status；
- `__bang_printf("<格式化字符串>", <参数变量>)`：将字符串打印到屏幕上；
- `__breakdump_scalar/vector()`：将标量/向量值dump到指定文件；

#### 标量调试

- BANG用户可通过 `__breakdump_scalar` 函数打印标量值。其语义是将一组用户指定的标量变量打印到 `.dumpscalar_data` 文件中，并终止在当前核上程序运行。主要的使用格式：
	1. `__breakdump_scalar(var1)`：dump一个标量的值；
	2. `__breakdump_scalar(var1, var2)`：同时dump两个标量的值；
	3. `__breakdump_scalar(var1，var2, var3)`：同时dump三个标量的值；
	4. ……
- 每次可以同时dump最多6个标量的值；
- `__breakdump_scalar` 语句后，MLU中当前核会终止程序的执行；
- 需设置环境变量 `DUMPMLUSCALAR` 才能将数据dump到 `.dumpscalar_data` 中；

#### 向量调试

- BANG用户可通过 `__breakdump_vector` 函数打印向量值。其语义是将一组用户指定的向量的值打印到 `.dumpsvector_data` 文件中，并终止在当前核上程序运行：
  - `__breakdump_vector(void* src, int32_t Bytes, mluBreakDumpAddrSpacE_t AddrSpace)`
  - 参数 `AddrSpace` 表示需要dump的地址数据所在的地址空间，目前BANG支持nram/ldram空间上向量数据的dump功能，因此 `AddrSpace` 取值为**NRAM/LDRAM**；
- 目前支持一次最多dump 1024字节，即 `Bytes <= 1024`；
- 需设置环境变量 `DUMPMLUVECTOR` 才能将数据dump到 `.dumpvector_data` 中；

![Example_of_Vector_Debugging](./Images/8.6-1-Example_of_Vector_Debugging.png)

#### 格式化输出

- BANG用户可通过 `__bang_printf` 函数打印零个或多个标量的值。其语义是将参数按照格式化字符串打印参数到屏幕（标准输出）；
- 该接口的格式如下：
	1. `__bang_printf(const char* fmt)`；
	2. `__bang_printf(const char* fmt, type arg1)`；
	3. `__bang_printf(const char* fmt, type arg1, type arg2)`；
	4. ……
- 并不终止程序的运行；

### 功能调试工具

面向智能编程语言BCL的调试器。

整体流程：调试前准备、调试器托管、状态查看及错误分析。

#### 调试前准备

- 配置调试目标设备号，配置设备号为1的命令：

```shell
export DLP_VISIBLE_DEVICES = 1
```

- 增加调试信息，编译时（编译器为bclc）使用 `-g` 选项：

```shell
bclc -g foo.dlp -o foo
bclc -g1 foo.dlp -o foo
bclc -g2 foo.dlp -o foo
bclc -g3 foo.dlp -o foo
```

#### 调试器托管

- 通过调试器执行被调试程序来实现托管。由于是异构程序，必须从主机端程序启动，若要在设备端Kernel程序的入口处停住，可以使用：

```shell
(bcl-gdb) bcl-gdb breakpoint on # 使能设备端的断点功能
(bcl-gdb) break *0x1 # 在Kernel函数的第一条指令上设置断点
(bcl-gdb) run # 执行程序
```

- 针对多核架构DLP考虑在不同核间切换，可以用 `info` 命令查看当前调试焦点并使用 `focus` 命令进行切换；

![Debugger_Deposit](./Images/8.6-2-Debugger_Deposit.png)

- 调试器托管：采用 `break` 命令可以根据函数名、代码行号、指令地址以及Kernel入口来增加断点。在 `break` 命令中可以**使用if语句来配置条件断点**。断点的查看和删除可分别使用 `info` 与 `delete` 命令来完成；

![Conditional_Breakpoint](./Images/8.6-3-Conditional_Breakpoint.png)

- 状态查看
	- 查看变量内容：调试时可直接根据变量名采用 `print` 命令来打印相关内容：

  ```shell
  (bcl-gdb) print a
  	$1 = 233
  (bcl-gdb) print array
  	$2 = (1,2,3,4)
  ```

	- 查看寄存器内容：寄存器内容则可采用 `info registers` 命令进行查看；
	- 查看地址内容：指定地址中的数据内容可通过 `examine` （或 `x`）命令进行查看：

  ```shell
  (bcl-gdb) x/2wd 0x2333
  	0x2333:666 888
  ```

### 功能调试实践

以智能编程语言BCL和相应调试器BCL-GDB为例介绍串行及并行程序的调试示例。

- BCL实现的快排程序（上：核心代码， 下：入口函数）：

![Quick_Sort_in_BCL](./Images/8.6-4-Quick_Sort_in_BCL.png)

- 串行调试程序
	- 使用设备端编译器编译出带调试信息的设备端二进制 `kernel.o`，并用gcc编译链接出主机端的可执行程序；

  ![Serial_Debugging](./Images/8.6-5-Serial_Debugging.png)

  - 采用 `backtrace` 命令查看当前函数的调用栈：

  ![Serial_Debugging_Backtrace](./Images/8.6-6-Serial_Debugging_Backtrace.png)

  - 使用 `layout src` 命令查看源码和当前断点：

  ![Serial_Debugging_Layout_Src](./Images/8.6-7-Serial_Debugging_Layout_Src.png)

  - 使用 `display` 命令和单步执行观察变量状态：

  ![Step_Execution](./Images/8.6-8-Step_Execution.png)

  - 使用 `up` 命令返回上一级调用栈，然后打印NRAM地址空间变量nBuff中数据：

  ![Return_to_Last_Stack](./Images/8.6-9-Return_to_Last_Stack.png)

- 并行程序调试：以 `kernel.dlp` 为例，可通过4个核并行执行该程序，主机端调用时设置UNION1任务执行：

  ![Parallel_Debugging](./Images/8.6-10-Parallel_Debugging.png)

	 通过 `break`、`continue`、`info`、`focus`、`list` 等命令观察多核计算的执行及调试核心的切换：

  ![Parallel_Debugging2](./Images/8.6-11-Parallel_Debugging2.png)

## 8.7 智能应用功能调优

### 性能调优方法
- 核心是如何充分利用**大规模的并行计算单元**；
- 尽量使用片上存储：

![Optimization_of_Store_on_Chip](./Images/8.7-1-Optimization_of_Store_on_Chip.png)

- 张量计算：张量化的基本原理是将**大量标量计算合并为张量计算**，使用智能编程语言的张量计算语句改写代码，充分利用硬件的张量计算单元，提升程序运行速度：

![Optimization_of_Tensor_Calculation](./Images/8.7-2-Optimization_of_Tensor_Calculation.png)

- 多核并行：针对（程序员可见的）多核，可将一个任务分拆到多个核上并行计算，进一步提升程序性能：

![Optimization_of_Parallel_Run](./Images/8.7-3-Optimization_of_Parallel_Run.png)

![Example_of_Performance_Counter_Interfaces](./Images/8.7-6-Example_of_Performance_Counter_Interfaces.png)

### 性能调优接口
- 在程序中使用性能调优接口，帮助识别程序执行过程的瓶颈；
- 识别瓶颈的过程：
	- 先找到耗时长的部分->**通知(Notifier)接口**；
	- 通过硬件计数器(Performance Counter)分析硬件执行特征->**硬件计数器接口**：

![Optimization_Interfaces](./Images/8.7-4-Optimization_Interfaces.png)

- 硬件计数器接口：通过提供硬件计数器值的获取接口，方便开发者对程序的行为进行**细粒度剖析和优化**：

![Performance_Counter_Interfaces](./Images/8.7-5-Performance_Counter_Interfaces.png)

### 性能调优工具
- 在**程序外部**监控程序的运行状态，分析其执行瓶颈，找到优化空间：
	- 应用级性能剖析工具；
	- 系统级性能监控工具；

- 应用级性能剖析工具，具体使用流程分为两个阶段：
	1. 采用`record` 命令运行可执行程序并生成相应的性能分析报告；
	2. 采用`report` 或`replay` 命令查看性能分析报告，获取包括**执行时间**、**调用关系**及**性能计数器**等信息；
	- 例如1，运行`record vecMult ./info_dir`、`report info_dir`：
	
	![Example_of_Optimization_Tool_of_App_Level](./Images/8.7-7-Example_of_Optimization_Tool_of_App_Level.png)
	
	- 例如2，运行`replay info_dir`，以时间线的方式来查看函数的执行时间：

	![Example_of_Optimization_Tool_of_App_Level2](./Images/8.7-8-Example_of_Optimization_Tool_of_App_Level2.png)

- 系统级性能监控工具，主要利用驱动通过读取寄存器的方式来收集硬件的静态与动态信息

| 命令示例 | 具体功能 |
| :--------: | :------: |
| `monitor -info` | 显示以下所有信息 |
| `monitor -type` | 显示板卡型号 |
| `monitor -driver` | 显示驱动版本 |
| `monitor -fan` | 显示风扇转速比 |
| `monitor -power` | 显示运行功耗 |
| `monitor -temp` | 显示芯片温度 |
| `monitor -memory` | 显示物理内存使用情况 |
| `monitor -bandwidth` | 显示计算核心对设备内存的最大访问带宽 |
| `monitor -core` | 显示各计算核心的利用率 |

### 性能调优实践
- 将前述系列方法应用到DFT的性能调优

DFT是将信号从时域变换到频域的算法，且时域和频域都是离散的。

$$X(k)=\sum^{N-1}_{n=0}x(n)e^{-j\cfrac{2\pi}{N}kn}$$

其中， $X(k)$是变换后的频域序列， $x(n)$是变换前的时域序列（采样信号，均为实数，无虚部）。如果将上式的 $e^{-j\cfrac{2\pi}{N}kn}$部分，用欧拉公式替换，可得到如下公式：

$$X(k)=\sum^{N-1}_{n=0}x(n)[\cos(-\dfrac{2\pi}{N}kn)+j\sin(-\dfrac{2\pi}{N}kn)]$$

$$=\sum^{N-1}_{n=0}[x(n)\cos(\dfrac{2\pi}{N}kn)-jx(n)\sin(\dfrac{2\pi}{N}kn)]$$

其中， $X(k)$的实部和虚部分别为：

$$real(k)=\sum^{N-1}_{n=0}x(n)\cos(\dfrac{2\pi}{N}kn)$$
$$imag(k)=\sum^{N-1}_{n=0}-x(n)\sin(\dfrac{2\pi}{N}kn)$$

由此得到 $X(k)$的幅值为：

$$Amp(k)=\sqrt{real(k)^2+image(k)^2}$$

- 首先用标量实现上述DFT基础算法

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* Amp) {
	for (int k = 0; k < N; k++) {
		float real = 0.0;
		float imag = 0.0;
		for (int n = 0; n < N; n++) {
			real += x[n] * cos(2 * PI / N * k * n);
			imag += -x[n] * sin(2 * PI / N * k * n);
		}
		Amp[k] = sqrt(real * real + imag * imag);
	}
}
```

- 使用**片上缓存**优化

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* Amp) {
	__nram__ float in[N];
	__nram__ float out[N];
	__memcpy(in, x, N * sizeof(float), GDRAM2NRAM);
	
	for (int k = 0; k < N; k++) {
		float real = 0.0;
		float imag = 0.0;
		for (int n = 0; n < N; n++) {
			real += in[n] * cos(2 * PI / N * k * n);
			imag += -in[n] * sin(2 * PI / N * k * n);
		}
		out[k] = sqrt(real * real + imag * imag);
	}
	__memcpy(Amp, out, N * sizeof(float), NRAM2GDRAM);
}
```

- 将最后的幅值计算从外层循环中拿出，采用**向量优化**

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* Amp) {
	__nram__ float in[N];
	__nram__ float out[N];
	__nram__ float real[N];
	__nram__ float imag[N];
	__memcpy(in, x, N * sizeof(float), GDRAM2NRAM);
	
	for (int k = 0; k < N; k++) {
		real[k] = 0.0;
		imag[k] = 0.0;
		for (int n = 0; n < N; n++) {
			real[k] += in[n] * cos(2 * PI / N * k * n);
			imag[k] += -in[n] * sin(2 * PI / N * k * n);
		}
	}
	
	__vec_mul(real, real, real, N);
	__vec_mul(imag, imag, imag, N);
	__vec_add(out, real, imag, N);
	__vec_sqrt(out, out, N);
	__memcpy(Amp, out, N * sizeof(float), NRAM2GDRAM);
}
```

- **算法优化**
	1. 原始算法内 $\sin$和 $\cos$括号内值相同，所以计算一遍即可；
	2.  $2*\pi/N*k*n$这五个数中前三个是常数，不必重复计算，在循环外计算一次即可；
	3. $imag$因为后面要取平方，累加时不用取负数；

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* Amp) {
	__nram__ float in[N];
	__nram__ float out[N];
	__nram__ float real[N];
	__nram__ float imag[N];
	__memcpy(in, x, N * sizeof(float), GDRAM2NRAM);

	float con = 2 * PI / N;
	for (int k = 0; k < N; k++) {
		real[k] = 0.0;
		imag[k] = 0.0;
		for (int n = 0; n < N; n++) {
			float temp = con * k * n;
			real[k] += in[n] * cos(temp);
			imag[k] += in[n] * sin(temp);
		}
	}
	
	__vec_mul(real, real, real, N);
	__vec_mul(imag, imag, imag, N);
	__vec_add(out, real, imag, N);
	__vec_sqrt(out, out, N);
	__memcpy(Amp, out, N * sizeof(float), NRAM2GDRAM);
}
```

- 主要遗留 $real$与 $imag$计算未经优化，其中主要计算有两个：
	1. 内层`for` 循环中的`k * N` ；
	2. 内层`for` 循环中的`real[k] += in[n] * cos()` 与`imag[k] += in[n] * sin()`；对于两层`for` 循环中的`k * N`，一共会产生 $N^2$个结果，可以用矩阵乘法来表示计算结果：

	![Example_of_Algorithm_Optimization](./Images/8.7-9-Example_of_Algorithm_Optimization.png)
	 
	 这样`k * N` 的结果都保存在矩阵中，后续乘以 $2*\pi/N$的过程可用`__cycle_mul` 完成；
	 
	![Example_of_Algorithm_Optimization2](./Images/8.7-10-Example_of_Algorithm_Optimization2.png)

- 之后的变量`sin(temp)` 与`cos(temp)` 同样可替换为向量计算（`vec_sin` 与`vec_cos`）；最后剩下两层内层`for` 循环中的`real[k] += in[n] * cos()` 与`imag[k] += in[n] * sin()` 同样可转换为`in` 向量和 $N*N$矩阵的向量矩阵乘法运算。

![Example_of_Algorithm_Optimization3](./Images/8.7-11-Example_of_Algorithm_Optimization3.png)

- **算法优化**结果

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* Amp) {
	__nram__ float in[N];
	__nram__ float real[N];
	__nram__ float imag[N];
	__nram__ float con[1];
	__nram__ float sequence[N];
	__nram__ float kn[N * N];
	__nram__ float cos_res[N * N];
	
	__memcpy(in, x, N * sizeof(float), GDRAM2NRAM);
	con[0] = 2 * PI / N;

	for (int k = 0; k < N; k++) {
		sequence[k] = k;
	}

	__mat_mlu(kn, sequence, sequence, N, 1, N); // 构造k*n矩阵
	__cycle_mul(kn, kn, con, N * N, 1);         // 把k*n矩阵乘以常量con
	__vec_cos(cos_res, kn, N * N);              // 计算cos(kn)
	__vec_sin(kn, kn, N * N);                   // 计算sin(kn)，结果保存至kn
	__mat_mul(real, in, cos_res, 1, N, N);      // 计算实部real向量
	__mat_mul(imag, in, kn, 1, N, N);           // 计算虚部imag向量
	
	__vec_mul(real, real, real, N);
	__vec_mul(imag, imag, imag, N);
	__vec_add(imag, real, imag, N);             // real和imag的求和结果仍保存在imag 
	__vec_sqrt(imag, imag, N);
	__memcpy(Amp, imag, N * sizeof(float), NRAM2GDRAM);
}
```

- **常数预处理**：对于值固定的部分，可提前算好，对于处理大量不同输入时不必重复计算。

```cpp
#define PI 3.14159265
#define N 128

__dlp_entry__ void DFT(float* x, float* cos_mat, float* sin_mat, float* Amp) {
	__nram__ float in[N];
	__nram__ float real[N];
	__nram__ float imag[N];
	__nram__ float cos_res[N * N];
	__nram__ float sin_res[N * N];
	
	__memcpy(in, x, N * sizeof(float), GDRAM2NRAM);
	__memcpy(cos_res, cos_mat, N * N * sizeof(float), GDRAM2NRAM); // cos_mat提前计算好的
	__memcpy(sin_res, sin_mat, N * N * sizeof(float), GDRAM2NRAM); // sin_mat提前计算好的

	__mat_mul(real, in, cos_res, 1, N, N);      // 计算实部real向量
	__mat_mul(imag, in, sin_res, 1, N, N);      // 计算虚部imag向量
	
	__vec_mul(real, real, real, N);
	__vec_mul(imag, imag, imag, N);
	__vec_add(imag, real, imag, N);             // real和imag的求和结果仍保存在imag 
	__vec_sqrt(imag, imag, N);
	__memcpy(Amp, imag, N * sizeof(float), NRAM2GDRAM);
}
```

- 优化结果分析
	- 都是张量运算且使用NRAM，DFT的运算整体性能提升近1800倍；
	- 核心目的是**充分利用硬件的计算资源**
		- 充分**利用近端存储**，降低访存延迟带来的计算单元空闲；
		- 充分**利用张量计算单元**；
		- 充分**降低计算量**、节省存储空间（通过算法优化）；
		- 充分**利用多核并行**，利用多核的计算单元；

| 优化方法 | 性能提升倍数 |
| :--------: | :------: |
| 原始标量程序 | 1（基准性能） |
| 使用片上缓存 | 1.49 |
| 张量化 | 10.65 |
| 算法优化 | 29.27 |
| 常数预处理 | 1794 |

## 8.8 基于智能编程语言的系统开发

### 轻量级高性能库算子开发

#### 高性能库算子开发
- 高性能库（如MKL、cuDNN、CNML等）提供了常见算子在特定平台上的高性能实现，方便用户以API的形式直接调用；
- 如何用智能编程语言拓展高性能库算子，关键在于：
	- Kernel代码逻辑开发与优化；
	- 高性能库算子接口API的使用；
	- 流程：

![Development_of_High_Performance_Library_Operator](./Images/8.8-1-Development_of_High_Performance_Library_Operator.png)

#### 自定义算子集成
- 算子集成API介绍
	- 典型高性能库中自定义算子集成的主要API，主要包括自定义算子的创建(CreateCustomizedOp)，（前向）计算(ComputeCustomizedOp)和销毁(DestroyBaseOp)等接口；
- 基础算子集成：作为自定义的单个基础算子集成进高性能库中执行；

![Basic_Op](./Images/8.8-2-Basic_Op.png)

- 融合算子集成：作为自定义算子和高性能库中已有算子进行**融合计算**
	- 减少中间数据片外交换，提高性能；
	- 主要使用方式；

![Fusion_Op](./Images/8.8-3-Fusion_Op.png)

### PluginOp简介
- PluginOp接口主要在软件栈的CNML层提供，将用户用BANG实现的算子“插入”到CNML库中；
- 自定义算子尅与CNML中已有算子实现（如Conv、Pooling等）执行逻辑统一，从而利用CNML的特性（如融合执行、离线指令生成等）；

![PluginOp](./Images/8.8-4-PluginOp.png)

#### PluginOp算子集成：基础算子

![BasicOp_in_PluginOp](./Images/8.8-5-BasicOp_in_PluginOp.png)

#### PluginOp算子集成：融合算子

![FusionOp_in_PluginOp](./Images/8.8-6-FusionOp_in_PluginOp.png)

### 编程框架算子开发

#### TensorFlow集成自定义算子
通过高性能库自定义算子API，可以将用编程语言开发的新算子集成到TF等编程框架中；在框架中的主要流程包括：
1. 为新的自定义算子进行注册；
2. 为新的自定义算子编写前向传播接口函数；
3. 根据新的自定义算子接口编写定制算子底层实现；
4. 完善Bazel Build和头文件，并重新编译TF源码；

## 8.9 小结
通过智能编程语言+编程框架提供**高开发效率**、**高性能**和**高可移植**的编程方法

![Summary](./Images/8.9-1-Summary.png)