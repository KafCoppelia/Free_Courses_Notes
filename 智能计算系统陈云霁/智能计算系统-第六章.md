# Chapter 6 深度学习处理器原理
Revision：1

## 6.1 深度学习处理器概述
### 研究意义
大而深的网络
![Deep_Network](./Images/6.1-1-Deep_Network.png)
为什么需要深度学习处理器？
- 深度学习应用广泛：
	- 图像识别、语音处理、自然语言处理、博弈游戏等领域；
	- 已渗透到云服务器和智能手机的方方面面；
- 通用CPU/GPU处理人工神经网络效率低下：
	- 谷歌大脑：1.6万个CPU核跑数天完成猫脸识别训练；
	- AlphaGo：下棋用了1202个CPU和200个GPU；

- 图像处理->GPU
- 信号处理->DSP
- 智能处理->?
- 未来每台计算机可能都需要一个专门的深度学习处理器：
	- 从云服务器到智能手机；
	- 应用面将超过GPU：每年**数十亿**片；

### 发展历史
- 第一次热潮（1950年代～1960年代）
	- 1951，M. Minsky研制了神经网络模拟器SNARC；
	- 1960，F. Rosenblatt研制了神经网络计算器Mark-I；深度学习处理器发展的三个因素：
![First_Climax](./Images/6.1-2-First_Climax.png)
- 第二次热潮（1980年代～1990年代初）
	- 1989，Intel ETANN；
	- 1990，CNAPS；
	- 1993，MANTRA I；
	- 1997，预言神；
	- ……
- 第三次热潮（2006～至今）

1990s的神经网络处理器
- 结构简单
- 规模小
![ETANN](./Images/6.1-3-ETANN.png)
![Other_DLPs](./Images/6.1-4-Other_DLPs.png)
![Third_Climax](./Images/6.1-5-Third_Climax.png)

深度处理器发展的三个因素：
1. 架构
2. 技术
3. 应用

### 设计思路
深度学习处理器的定位：
- 通用性稍低于GPU，能效高于GPU；
- 通用性比ASICs高，但能效稍低于ASICs；
![Position_of_DLP](./Images/6.1-6-Position_of_DLP.png)

如何设计一个深度学习处理器DLP：
- 目标？
- 体系结构？
- 微体系结构？
- 可编程性？

设计思路：
- 整个体系结构最重要的问题：
	- 算法范围界定；
	- 算法分析（计算特性、访存特性）；
- 谁是我们的“朋友”？
	- 自定制硬件，可利用算法特性；
	- 高效率；
- 谁是我们的“敌人”？
	- 阻碍高效率：带宽、访存速度、访存代价；

## 6.2 目标算法分析
*VGG19为例*
![Structure_of_VGG19](./Images/6.2-7-Structure_of_VGG19.png)
### 分析什么？
体系结构设计人员应分析什么？
- 计算
	- 是否存在固定重复的计算模式；
- 访存
	- 数据的局部性；
	- 数据和计算的关系（对于带宽的需求）；

### 全连接层
![Layer_of_FC](./Images/6.2-8-Layer_of_FC.png)
$$\boldsymbol{y}[j]=G\left(\boldsymbol{b}[j]+\sum_{i=0}^{N_i-1}\boldsymbol{W}[j][i]\times\boldsymbol{x}[i]\right)$$
- 代码实现：
```cpp
// x是输入神经元，y是输出神经元，W是权重
y(all) = 0; // 初始化所有输出神经元
	for (j=0; i<No; i++) //外循环复用，复用距离等于Ni
	{
		for (i=0; i<Ni; i++) //内循环复用，距离=1
		{
			y[j] += W[j][i]*x[i]; //内外循环无复用
			if (i==Ni)
			  	y[j] = G(y[j]+b[j]);
		}
  	}
}
```
- 计算特点
	- 向量内积、向量的元素操作；
	- 无复杂控制流；

### 卷积层
- VGG19中第一层
![First_Conv_Layer_of_VGG19](./Images/6.2-9-First_Conv_Layer_of_VGG19.png)
$$\boldsymbol{y}[nor][moc][j]=G\left(\boldsymbol{b}[j]+\sum_{i=0}^{N_{if}-1}\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}\boldsymbol{W}[k_r][k_c][j][i]\times\boldsymbol{X}[r+k_r][c+k_c][i]\right)$$
- 代码实现：
  ```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (j=0; j<Nof; j++)
			sum[j] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (j=0; j<Nof; j++)
					for (i=0l; i<Nif; i++)
						sum[j] += W[kr][kc][j][i]*X[r+kr][c+kc][i];
		for (j=0; j<Nof; j++)
			Y[nor][noc][j] = G(sum[j]+b[j]);
		noc++;
	}
	nor++;
}
  ```
- 计算特点
	- 矩阵内积、向量的元素操作；
	- 无复杂控制流；

### 池化层
![Pooling_Layers](6.2-10-Pooling_Layers.png)
- Max Pooling：
$$\boldsymbol{Y}[nor][noc][i]=\max_{0\le k_c\le K_c,0\le k_r\le K_r}(\boldsymbol{X}[r+kr][c+kc][i])$$
- AVG Pooling：
$$\boldsymbol{Y}[nor][noc][i]=\dfrac{1}{K_c\times K_r}\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}(\boldsymbol{X}[r+kr][c+kc][i])$$
- 代码实现：
  ```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (i=0; i<Nif; i++)
			value[i] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (i=0; i<Nif; i++) {
					// for average pooling
					value[i] += X[r+kr][c+kc][i];
					// for max pooling
					value[i] +=max(value[i], X[r+kr][c+kc][i]);
				}
		for (i=0; i<Nif; i++) {
			// for average pooling
			Y[nor][noc][i] = value[i]/Kr/Kc;
			// for max pooling
			Y[nor][noc][i] = value[i];
		}
		noc++;
	}
	nor++;
}
  ```
- 计算特点：
	- 向量的元素操作；
	- 无复杂控制流；

### CNN计算特征
不同层的计算特点：
| 层 | 计算类型 | 乘加操作个数 | 激活函数操作个数 |
| :------: | :--------------------------------------: | :-------------------------------: | :-------------------------------: |
|  卷积层 |  矩阵内积、向量的元素操作 | $N_{if}\times N_{of}\times N_{or}\times N_{oc}\times K_r\times K_c$个乘加 | $N_{of}\times N_{or}\times N_{oc}$ |
|  池化层 | 当向量的元素操作 | $N_{if}\times N_{or}\times N_{oc}\times {K_r}\times K_c$个加法或比较+$N_{if}\times N_{or}\times N_{oc}$个除法操作（平均池化） | 无 |
| 全连接层 | 矩阵乘向量、向量的元素操作 | $N_o\times N_i$个乘加 | $N_o$ |

访存特征：
- 全连接层：可解耦性、可复用性；
![全连接层访存特性](6.2-11-全连接层访存特性.png)
- 权值数据量大无复用，带宽需求高，需进行循环分块（可对卷积层、池化层）；
![全连接层访存特性2](./Images/6.2-12-全连接层访存特性2.png)
![Tiling](./Images/6.2-13-Tiling.png)
- 全连接层做两层循环分块(tiling)
```c
for (jjj=0; jjj<No; jjj+=Tjj) { // 对输出神经元进行分块，Tjj和Tj是两层分块大小
	for (jj=jjj; jj<jjj+Tjj; jj+=Tj) {
		for (j=jj; j<jj+Tj; j++)
			y[j] = 0;
		for (iii=0; iii<Ni; iii+=Tii) { // 对输入神经元进行分块，Tii和Ti是两层分块大小
			for (ii=iii; ii<iii+Tii;ii+=Ti)
				for (j=jj; j<jj+Tj; j++)
					for (i=ii; i<ii+Ti; i++)
						sum[j] += W[j][i]*x[i];
		}
		for (j=jj; j<jj+Tj; j++)
			y[j] = G(sum[j] + b[j]);
	}
}
```
![Tiling_of_FC](6.2-14-Tiling_of_FC.png)

不同层的重用特性：
|  层  |  可重用  |  不可重用  |
|  :----------:  |  :------------:  |  :-----------:  |
|  卷积层  |  输入神经元、输出神经元、突触权重   |  无   |
|  池化层  | 当池化窗口大于步长，部分输入神经元可重用 | 池化窗口小于等于步长时，输入神经元、输出神经元均不可重用  |
|  全连接层  |   输入神经元、输出神经元  |   突触权重  |

## 6.3 深度学习处理器DLP结构
### DLP结构 
- 指令集
- 流水线
- 运算部件
- 访存部件
- 算法映射
![Layers_of_Abstraction](./Images/6.3-15-Layers_of_Abstraction.png)

### 指令集
- 计算机的抽象模型
	- 定义了体系结构；
	- 软硬件的唯一接口；
- 为什么采用指令集？
	- 灵活性：支持未来可能出现的新的深度学习算法；
	- 通用性：支持广泛的深度学习算法；
- 设计原则
	- Data-level Parallelism；
	- 可向量化操作；
	- Load-store结构：只通过load和store指令访问主存；
	- 64位定长指令，变长操作数（寄存器指定长度）；
- DLP指令集
![Instruction_Set_of_DLP](./Images/6.3-16-Instruction_Set_of_DLP.png)

- 控制指令
	- JUMP：立即跳转指令
	- CB：条件分支指令
- 计算指令
	- 矩阵运算：MMV、VMM、MMS、OP（外积）、MAM、MSM
	- 向量运算：VAV、VSV、VMV、VDV、VEXP、VLOG、IP（内积）、RV（随机向量生成）
	- 标量运算：加减乘除基本运算，标量超越函数
- 逻辑指令
	- 向量逻辑：比较(VGT、VE)，逻辑(VAND、VOR、VNOT)，最值归约(VGTM)
	- 标量逻辑：标量比较，标量逻辑运算；

#### DLP代码示例
- 全连接层
![Example_of_FC_Layer](./Images/6.3-17-Example_of_FC_Layer.png)
- 池化层
![Example_of_Pooling_Layer](./Images/6.3-18-Example_of_Pooling_Layer.png)

### 流水线
![Pipeline](./Images/6.3-19-Pipeline.png)
- 7段流水：取值、译码、发射、读寄存器、执行、写回、提交；
- 前半节基本为通用CPU架构，后半节为深度学习加入VFU、MFU及三个RAM；

#### 运算部件
- MAC(Multiply-Accumulator)：标量MAC单元 vs. 向量MAC单元
![Scalar_MAC_vs_Vector_MAC](./Images/6.3-20-Scalar_MAC_vs_Vector_MAC.png)
- N个向量MAC单元堆叠；
- 能够支撑DLP指令集：矩阵/向量/标量计算指令；
- 可否完成：全连接层/池化层/卷积层？
- VGG19中三种层
![Support_of_VGG19](./Images/6.3-21-Support_of_VGG19.png)
![Unsupport_of_VGG19](./Images/6.3-22-Unsupport_of_VGG19.png)
- 不支持的主要有：
	- 激活函数处理单元：非线性函数单元；
	- 池化操作：MFU的三个stage的退出通路；
	- 任意规模：局部累加功能；
![Summary_of_Calculation_Components](./Images/6.3-23-Summary_of_Calculation_Components.png)

#### 访存部件
![Energy_Consumption_of_Load_and_Store](./Images/6.3-24-Energy_Consumption_of_Load_and_Store.png)
- 可解耦性
  - 三个分离访存部件：NRAM-in，NRAM-out，WRAM；
  - 有效避免访存流之间互相干扰
- 可复用性
  - 片上缓存：形成“运算单元-片上-片外”的存储架构；
  - Scratchpad Memory管理；
  - 提高片上数据复用率；

### 算法映射
- 基本思想：硬件**分时复用**；
![TDM_of_FC_Layers1](./Images/6.3-25-TDM_of_FC_Layers1.png)
![TDM_of_FC_Layers2](./Images/6.3-26-TDM_of_FC_Layers2.png)
- 全连接层映射：具体计算指令的顺序；
![Algorithm_Reflection_of_FC_Layers](./Images/6.3-27-Algorithm_Reflection_of_FC_Layers.png)
- 卷积层映射
![Algorithm_Reflection_of_Conv_Layers](./Images/6.3-28-Algorithm_Reflection_of_Conv_Layers.png)
|  图示  |  描述  |
|  :----------:  |  :------------:  |
|  ![Algorithm_Reflection_of_Conv_Layers2](./Images/6.3-29-Algorithm_Reflection_of_Conv_Layers2.png)  |  $N$个向量MAC计算$N$个输出神经元，其中这些输出神经元属于$N$个输出feature map。片内载入数据是所有输入feature map的第一行（$Nic\times Nif$个输入），而正在计算的这$N$个输出神经元只和$Kc\times Nif$个输入相关，所需要的权值是$Kc\times Nif\times N$个   |
|  ![Algorithm_Reflection_of_Conv_Layers3](./Images/6.3-30-Algorithm_Reflection_of_Conv_Layers3.png)  | 当计算完成$N$个输出神经元中与$Kc\times Nif$个输入相关的部分，开始下面$N$个输出神经元。同样的，这$N$个神经元属于$N$个不同的输出feature map。在这个计算过程中，上$N$个输出神经元的$Kc\times Nif\times N$个权值可以复用 
| ![Algorithm_Reflection_of_Conv_Layers4](./Images/6.3-31-Algorithm_Reflection_of_Conv_Layers4.png)  |   当开始计算不同位置的$N$个神经元时，所依赖的输入与之前输入重叠，而权值则可以完全复用  |
| ![Algorithm_Reflection_of_Conv_Layers5](./Images/6.3-32-Algorithm_Reflection_of_Conv_Layers5.png) | 重复以上过程，只要计算的输出神经元属于同一个面($Nic\times Nof$)，所依赖的权值就不会发生变化 |
|![Algorithm_Reflection_of_Conv_Layers6](./Images/6.3-33-Algorithm_Reflection_of_Conv_Layers6.png) | 当与一个面的输入神经元计算都完成（即所有输入feature mape的第一行），后续计算已经不再需要这些输入神经元，片上可重新载入第二个面（即所有输入feature map的第二行），开始进行新的输出神经元的计算。重复以上过程 |
- 池化层映射；*某种意义上，是对卷积层的简化*
![Algorithm_Reflection_of_Pooling_Layers](./Images/6.3-34-Algorithm_Reflection_of_Pooling_Layers.png)
池化层的计算顺序与卷积层一致，只是计算是$\max()$或$\mathrm{avg}()$；

## 6.4 优化设计
### 基于标量MAC的运算部件
将普通标量MAC堆叠成MAC阵列，有几种连接方式
![Scalar_MAC_to_Vector_MAC](./Images/6.4-35-Scalar_MAC_to_Vector_MAC.png)
![Vector_MAC](./Images/6.4-36-Vector_MAC.png)
- 卷积运算中的数据复用
![Reuse_in_Conv](./Images/6.4-37-Reuse_in_Conv.png)
![A_Realization_of_Reusing_PEs](./Images/6.4-38-A_Realization_of_Reusing_PEs.png)
- 对比：向量MAC单元与标量MAC单元
|  | 基于向量MAC计算单元 | 基于标量MAC计算单元 | 
| :------: | :--------------------------------------: | :-------------------------------: |
| 大小 |  16个16维向量MAC | 256个MAC，16x16阵列 |
| 乘法器个数 | 256 | 256 |
| 加法器个数 | 240(16x15) | 256 |
| 每拍需求外部操作数 | 512 | 32 |
| 操作粒度 | 向量、矩阵 | 向量、矩阵 |
| 卷积层映射 | 输入神经元复用、输出神经元复用 | 输入神经元复用、输出神经元复用、权重复用 |
| 优点 | 高效支持矩阵向量映射，灵活性高 | 专用数据流高效支持卷积，带宽需求降低 |
| 缺点 | 依赖外部数据排布，带宽需求高 | 灵活性差，支持其他算子其他特性困难 |
- 非基于MAC运算单元
![Non-MAC](./Images/6.4-39-Non-MAC.png)

### 稀疏化
![Sparse_Synapsis](./Images/6.4-40-Sparse_Synapsis.png)
```mermaid
flowchart LR
	原始网络 --> 网络训练 --> 粗粒度减枝与重训练 --> 局部量化 --> 熵编码 --> 压缩网络
```

### 低位宽
![Low_Bit_Width](./Images/6.4-41-Low_Bit_Width.png)

### 性能评价
- TOPS(Tera Operations Per Second)
$$TOPS = f_c(\mathrm{GHz})\times (N_{mul}+N_{add})/1000$$
- 访存带宽：主频$f_m$、存储位宽$b$、访存效率$\eta$
$$BW=f_m\times b\times\eta$$
- 基准测试程序：MLPerf
![MLPerf_Baselines](./Images/6.4-42-MLPerf_Baselines.png)

### 影响性能的因素
$$T=\sum_i N_i\times C_i/f_c$$
- $N_i$表示该任务中第$i$类操作的数量；
- $C_i$表示完成第$i$类操作需要的时钟周期数；
- $f_c$处理器主频；

优化：
- 减小$C_i$；
- 减少访存开销；
- 多级并行；

## 6.5 其他加速器
- GPU
	- 计算：SIMD(SIMT)
	- 存储：多层次
	- 控制：SIMD指令
- FPGA
	- 计算：可配置CLB
	- 存储Block RAM
	- 控制：配置

| 类别 |       目标       | 速度 | 能效 |      灵活性      |
| :--: | :--------------: | :--: | :--: | :--------------: |
| DLP  |   深度学习专用   |  高  |  高  | 深度学习领域通用 |
| FPGA | 通用的可编程电路 |  低  |  中  |       通用       |
| GPU  | SIMD架构矩阵加速 |  中  |  低  |  矩阵类应用通用  |