# Chapter 6 深度学习处理器原理
Revision：1

## 6.1 深度学习处理器概述
### 研究意义
大而深的网络
![Deep_Network](./Images/6.1-1-Deep_Network.png)
为什么需要深度学习处理器？
- 深度学习应用广泛：
	- 图像识别、语音处理、自然语言处理、博弈游戏等领域；
	- 已渗透到云服务器和智能手机的方方面面；
- 通用CPU/GPU处理人工神经网络效率低下：
	- 谷歌大脑：1.6万个CPU核跑数天完成猫脸识别训练；
	- AlphaGo：下棋用了1202个CPU和200个GPU；

- 图像处理->GPU
- 信号处理->DSP
- 智能处理->?
- 未来每台计算机可能都需要一个专门的深度学习处理器：
	- 从云服务器到智能手机；
	- 应用面将超过GPU：每年**数十亿**片；

### 发展历史
- 第一次热潮（1950年代～1960年代）
	- 1951，M. Minsky研制了神经网络模拟器SNARC；
	- 1960，F. Rosenblatt研制了神经网络计算器Mark-I；深度学习处理器发展的三个因素：
![First_Climax](./Images/6.1-2-First_Climax.png)
- 第二次热潮（1980年代～1990年代初）
	- 1989，Intel ETANN；
	- 1990，CNAPS；
	- 1993，MANTRA I；
	- 1997，预言神；
	- ……

1990s的神经网络处理器
- 结构简单
- 规模小
![ETANN](./Images/6.1-3-ETANN.png)
![Other_DLPs](./Images/6.1-4-Other_DLPs.png)

1. 架构；
2. 技术；
3. 应用；

深度学习处理器的定位：通用性稍低于GPU，能效高于GPU；通用性比ASICs高，但能效稍低于ASICs；

#### 如何设计一个深度学习处理器DLP？

- 目标；
- 体系结构；
- 微体系结构；
- 可编程性；

设计思路：

- 最重要的问题：**体系结构**
  - 算法范围界定；
  - 算法分析（计算特性、访存特性）；
- 谁是朋友？
  - 自定制硬件，可利用算法特性；
  - 高效率；
- 谁是敌人？
  - 阻碍高效率：带宽、访存速度、访存代价；

## 6.2 目标算法分析

VGG19为例

#### 分析什么？

- 体系结构设计人员应分析什么？
- 计算
  - 是否存在固定重复的计算模式；
- 访存
  - 数据的局部性；
  - 数据和计算的关系（对于带宽的需求）；

#### 全连接层

- 代码实现：

  ```c
  y(all) = 0;
  for (j=0; i<Ni; i++) //外循环复用，复用距离等于Ni
  {
  	for (i=0; i<Ni; i++) //内循环复用，距离=1
  	{
  		y[j]+=W[j][i]*x[i]; //内外循环无复用
  		if (i=Ni)
  			y[j]=G(y[j]+b[j]);
  	}
  }
  ```

- 计算特点

  - 向量内积、向量的元素操作；
  - 无复杂控制流；

#### 卷积层

- 计算特点
  - 矩阵内积、向量的元素操作；
  - 无复杂控制流；

#### 池化层

- 计算特点：
  - 向量的元素操作；
  - 无复杂控制流；

#### CNN计算

总的计算特征：

- 矩阵内积
- 向量的元素操作
- 矩阵乘向量

访存特征：

- 全连接层：可解耦性、可复用性；

- 权值数据量大无复用，带宽需求高，需进行循环分块（可对卷积层、池化层）；

- 不同层的重用特性：

  |    层    |                  可重用                  |             不可重用              |
  | :------: | :--------------------------------------: | :-------------------------------: |
  |  卷积层  |        输入、输出神经元、突触权重        |                无                 |
  |  池化层  | 当池化窗口大于步长、部分输入神经元可重用 | 池化<步长，输入、输出神经元均不可 |
  | 全连接层 |             输入、输出神经元             |             突触权重              |

## 6.3 深度学习处理器DLP结构

DLP结构

- 指令集
- 流水线
- 运算部件
- 访存部件
- 算法映射

#### 指令集

- 计算机的抽象模型
  - 定义了体系结构；
  - 软硬件的唯一接口；
- 为什么采用指令集？
  - 灵活性：支持未来可能出现的新的深度学习算法；
  - 通用性：支持广泛的深度学习算法；

- 设计原则
  - Data-level Parallelism；
  - 可向量化操作；
  - Load-store结构：只通过load和store指令访问主存；
  - 64位定长指令，变长操作数（寄存器指定长度）；
- 控制指令、数据传输指令（Load/Store、MOVE）、计算指令（矩阵运算、向量运算、标量运算）、逻辑指令（向量逻辑、标量逻辑）

#### 流水线

- 7段流水：取值、译码、发射、读寄存器、执行、写回、提交

  ![image-20220317224438674](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20220317224438674.png)

- MAC(Multiply-Accumulator)：标量MAC单元与向量MAC单元

- 运算部件
  - N个向量MAC单元堆叠；
  - 能够支撑DLP指令集；
  - 激活函数处理单元：非线性函数单元；
  - 池化操作：MFU的三个stage的退出通路；
  - 任意规模：局部累加功能；

#### 访存部件

**非常关键**

- 可解耦性
  - 三个分离访存部件：NRAM-in，NRAM-out，WRAM；
  - 有效避免访存流之间互相干扰

- 可复用性
  - 片上缓存：形成“运算单元-片上-片外”的存储架构；
  - Scratchpad Memory管理；
  - 提高片上数据复用率；

#### 算法映射

- 基本思想：硬件**分时复用**
- 全连接层映射：具体计算指令的顺序；
- 卷积层映射；
- 池化层映射；

## 6.4 优化设计与性能评价

 #### 基本运算单元

- 卷积运算中的数据复用

- 稀疏化

  原始网络→网络训练→粗粒度减枝与重训练→局部量化→熵编码→压缩网络

- 低位宽
- 串行乘法器

#### 性能评价

- TOPS(Tera Operations Per Second)

  $TOPS = f_c(\mathrm{GHz})\times (N_{mul}+N_{add})/1000$

- 访存带宽

  $BW=f_m\times b\times\eta$

  $f_m$存储器主频，存储位宽$b$，访存效率$\eta$；

- 基准测试程序

  - MLPerf

#### 影响性能的因素

$T=\sum_i N_i\times C_i/f_c$

$N_i$表示该任务中第$i$类操作的数量；

$C_i$表示完成第$i$类操作需要的时钟周期数；

$f_c$处理器主频；

- 减小$C_i$；
- 减少访存开销；
- 多级并行；

## 6.5 其他加速器

| 类别 |       目标       | 速度 | 能效 |      灵活性      |
| :--: | :--------------: | :--: | :--: | :--------------: |
| DLP  |   深度学习专用   |  高  |  高  | 深度学习领域通用 |
| FPGA | 通用的可编程电路 |  低  |  中  |       通用       |
| GPU  | SIMD架构矩阵加速 |  中  |  低  |  矩阵类应用通用  |
