# Chapter 6 深度学习处理器原理

Revision：3

## 6.1 深度学习处理器概述

### 研究意义

大而深的网络：

![Deep_Network](./Images/6.1-1-Deep_Network.png)

为什么需要深度学习处理器？

- 深度学习应用广泛：

  - 图像识别、语音处理、自然语言处理、博弈游戏等领域；
  - 已渗透到云服务器和智能手机的方方面面；
- 通用CPU/GPU处理人工神经网络效率低下：

  - 谷歌大脑：1.6万个CPU核跑数天完成猫脸识别训练；
  - AlphaGo：下棋用了1202个CPU和200个GPU；
- 图像处理->GPU
- 信号处理->DSP
- 智能处理->?
- 未来每台计算机可能都需要一个专门的深度学习处理器：

  - 从云服务器到智能手机；
  - 应用面将超过GPU：每年**数十亿**片；

### 发展历史

- 第一次热潮（1950年代～1960年代）
  - 1951，M. Minsky研制了神经网络模拟器SNARC；
  - 1960，F. Rosenblatt研制了神经网络计算器Mark-I；深度学习处理器发展的三个因素：

![First_Climax](./Images/6.1-2-First_Climax.png)

- 第二次热潮（1980年代～1990年代初）
  - 1989，Intel ETANN；
  - 1990，CNAPS；
  - 1993，MANTRA I；
  - 1997，预言神；
  - ……
- 第三次热潮（2006～至今）

1990s的神经网络处理器：

- 结构简单
- 规模小

![ETANN](./Images/6.1-3-ETANN.png)

![Other_DLPs](./Images/6.1-4-Other_DLPs.png)

![Third_Climax](./Images/6.1-5-Third_Climax.png)

深度处理器发展的三个因素：

1. 架构
2. 技术
3. 应用

### 设计思路

深度学习处理器的定位：

- 通用性稍低于GPU，能效高于GPU；
- 通用性比ASICs高，但能效稍低于ASICs；

![Position_of_DLP](./Images/6.1-6-Position_of_DLP.png)

如何设计一个深度学习处理器DLP：

- 目标？
- 体系结构？
- 微体系结构？
- 可编程性？

设计思路：

- 整个体系结构最重要的问题：
  - 算法范围界定；
  - 算法分析（计算特性、访存特性）；
- 谁是我们的“朋友”？
  - 自定制硬件，可利用算法特性；
  - 高效率；
- 谁是我们的“敌人”？
  - 阻碍高效率：带宽、访存速度、访存代价；

## 6.2 目标算法分析

*VGG19为例*

![Structure_of_VGG19](./Images/6.2-1-Structure_of_VGG19.png)

### 分析什么？

体系结构设计人员应分析什么？

- 计算
  - 是否存在固定重复的计算模式；
- 访存
  - 数据的局部性；
  - 数据和计算的关系（对于带宽的需求）；

### 全连接层

![Layer_of_FC](./Images/6.2-2-Layer_of_FC.png)

$$
\boldsymbol{y}[j]=G\left(\boldsymbol{b}[j]+\sum_{i=0}^{N_i-1}\boldsymbol{W}[j][i]\times\boldsymbol{x}[i]\right)
$$

- 代码实现：

```cpp
// x是输入神经元，y是输出神经元，W是权重
y(all) = 0; // 初始化所有输出神经元
	for (j=0; i<No; i++) //外循环复用，复用距离等于Ni
	{
		for (i=0; i<Ni; i++) //内循环复用，距离=1
		{
			y[j] += W[j][i]*x[i]; //内外循环无复用
			if (i==Ni)
			  	y[j] = G(y[j]+b[j]);
		}
  	}
}
```

- 计算特点
  - 向量内积、向量的元素操作；
  - 无复杂控制流；

### 卷积层

- VGG19中第一层

![First_Conv_Layer_of_VGG19](./Images/6.2-3-First_Conv_Layer_of_VGG19.png)

$$
\boldsymbol{y}[nor][moc][j]=G\left(\boldsymbol{b}[j]+\sum_{i=0}^{N_{if}-1}\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}\boldsymbol{W}[k_r][k_c][j][i]\times\boldsymbol{X}[r+k_r][c+k_c][i]\right)
$$

- 代码实现：

```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (j=0; j<Nof; j++)
			sum[j] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (j=0; j<Nof; j++)
					for (i=0l; i<Nif; i++)
						sum[j] += W[kr][kc][j][i]*X[r+kr][c+kc][i];
		for (j=0; j<Nof; j++)
			Y[nor][noc][j] = G(sum[j]+b[j]);
		noc++;
	}
	nor++;
}
```

- 计算特点
  - 矩阵内积、向量的元素操作；
  - 无复杂控制流；

### 池化层

![Pooling_Layers](./Images/6.2-4-Pooling_Layers.png)

- Max Pooling：

$$
\boldsymbol{Y}[nor][noc][i]=\max_{0\le k_c\le K_c,0\le k_r\le K_r}(\boldsymbol{X}[r+kr][c+kc][i])
$$

- AVG Pooling：

$$
\boldsymbol{Y}[nor][noc][i]=\dfrac{1}{K_c\times K_r}\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}(\boldsymbol{X}[r+kr][c+kc][i])
$$

- 代码实现：

```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (i=0; i<Nif; i++)
			value[i] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (i=0; i<Nif; i++) {
					// for average pooling
					value[i] += X[r+kr][c+kc][i];
					// for max pooling
					value[i] +=max(value[i], X[r+kr][c+kc][i]);
				}
		for (i=0; i<Nif; i++) {
			// for average pooling
			Y[nor][noc][i] = value[i]/Kr/Kc;
			// for max pooling
			Y[nor][noc][i] = value[i];
		}
		noc++;
	}
	nor++;
}
```

-计算特点：
	- 向量的元素操作；
	- 无复杂控制流；

### CNN计算特征

不同层的计算特点：

|    层    |          计算类型          |                                                            乘加操作个数                                                            |           激活函数操作个数           |
| :------: | :------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------: |
|  卷积层  |  矩阵内积、向量的元素操作  |                             $N_{if}\times N_{of}\times N_{or}\times N_{oc}\times K_r\times K_c$个乘加                             | $N_{of}\times N_{or}\times N_{oc}$ |
|  池化层  |      当向量的元素操作      | $N_{if}\times N_{or}\times N_{oc}\times {K_r}\times K_c$个加法或比较 + $N_{if}\times N_{or}\times N_{oc}$个除法操作（平均池化） |                  无                  |
| 全连接层 | 矩阵乘向量、向量的元素操作 |                                                       $N_o\times N_i$个乘加                                                       |               $N_o$               |

访存特征：

- 全连接层：可解耦性、可复用性；

![全连接层访存特性](./Images/6.2-5-全连接层访存特性.png)

- 权值数据量大无复用，带宽需求高，需进行循环分块（可对卷积层、池化层）；

![全连接层访存特性2](./Images/6.2-6-全连接层访存特性2.png)

![Tiling](./Images/6.2-7-Tiling.png)

- 全连接层做两层循环分块(tiling)

```c
for (jjj=0; jjj<No; jjj+=Tjj) { // 对输出神经元进行分块，Tjj和Tj是两层分块大小
	for (jj=jjj; jj<jjj+Tjj; jj+=Tj) {
		for (j=jj; j<jj+Tj; j++)
			y[j] = 0;
		for (iii=0; iii<Ni; iii+=Tii) { // 对输入神经元进行分块，Tii和Ti是两层分块大小
			for (ii=iii; ii<iii+Tii;ii+=Ti)
				for (j=jj; j<jj+Tj; j++)
					for (i=ii; i<ii+Ti; i++)
						sum[j] += W[j][i]*x[i];
		}
		for (j=jj; j<jj+Tj; j++)
			y[j] = G(sum[j] + b[j]);
	}
}
```

![Tiling_of_FC](./Images/6.2-8-Tiling_of_FC.png)

不同层的重用特性：

|    层    |                  可重用                  |                         不可重用                         |
| :------: | :--------------------------------------: | :------------------------------------------------------: |
|  卷积层  |     输入神经元、输出神经元、突触权重     |                            无                            |
|  池化层  | 当池化窗口大于步长，部分输入神经元可重用 | 池化窗口小于等于步长时，输入神经元、输出神经元均不可重用 |
| 全连接层 |          输入神经元、输出神经元          |                         突触权重                         |

## 6.3 深度学习处理器DLP结构

### DLP结构

- 指令集
- 流水线
- 运算部件
- 访存部件
- 算法映射

![Layers_of_Abstraction](./Images/6.3-1-Layers_of_Abstraction.png)

### 指令集

- 计算机的抽象模型
  - 定义了体系结构；
  - 软硬件的唯一接口；
- 为什么采用指令集？
  - 灵活性：支持未来可能出现的新的深度学习算法；
  - 通用性：支持广泛的深度学习算法；
- 设计原则
  - Data-level Parallelism；
  - 可向量化操作；
  - Load-store结构：只通过load和store指令访问主存；
  - 64位定长指令，变长操作数（寄存器指定长度）；
- DLP指令集

![Instruction_Set_of_DLP](./Images/6.3-2-Instruction_Set_of_DLP.png)

- 控制指令
  - JUMP：立即跳转指令
  - CB：条件分支指令
- 计算指令
  - 矩阵运算：MMV、VMM、MMS、OP（外积）、MAM、MSM
  - 向量运算：VAV、VSV、VMV、VDV、VEXP、VLOG、IP（内积）、RV（随机向量生成）
  - 标量运算：加减乘除基本运算，标量超越函数
- 逻辑指令
  - 向量逻辑：比较(VGT、VE)，逻辑(VAND、VOR、VNOT)，最值归约(VGTM)
  - 标量逻辑：标量比较，标量逻辑运算；

#### DLP代码示例

- 全连接层

![Example_of_FC_Layer](./Images/6.3-3-Example_of_FC_Layer.png)

- 池化层

![Example_of_Pooling_Layer](./Images/6.3-4-Example_of_Pooling_Layer.png)

### 流水线

![Pipeline](./Images/6.3-5-Pipeline.png)

- 7段流水：取值、译码、发射、读寄存器、执行、写回、提交；
- 前半节基本为通用CPU架构，后半节为深度学习加入VFU、MFU及三个RAM；

#### 运算部件

- MAC(Multiply-Accumulator)：标量MAC单元 vs. 向量MAC单元

![Scalar_MAC_vs_Vector_MAC](./Images/6.3-6-Scalar_MAC_vs_Vector_MAC.png)

- N个向量MAC单元堆叠；
- 能够支撑DLP指令集：矩阵/向量/标量计算指令；
- 可否完成：全连接层/池化层/卷积层？
- VGG19中三种层：

![Supported_Layers_of_VGG19](./Images/6.3-7-Supported_Layers_of_VGG19.png)

![Unsupported_Layers_of_VGG19](./Images/6.3-8-Unsupported_Layers_of_VGG19.png)

- 不支持的主要有：
  - 激活函数处理单元：非线性函数单元；
  - 池化操作：MFU的三个stage的退出通路；
  - 任意规模：局部累加功能；

![Summary_of_Calculation_Components](./Images/6.3-9-Summary_of_Calculation_Components.png)

#### 访存部件

![Energy_Consumption_of_Load_and_Store](./Images/6.3-10-Energy_Consumption_of_Load_and_Store.png)

- 可解耦性
  - 三个分离访存部件：NRAM-in，NRAM-out，WRAM；
  - 有效避免访存流之间互相干扰
- 可复用性
  - 片上缓存：形成“运算单元-片上-片外”的存储架构；
  - Scratchpad Memory管理；
  - 提高片上数据复用率；

### 算法映射

- 基本思想：硬件**分时复用**；

![TDM_of_FC_Layers1](./Images/6.3-11-TDM_of_FC_Layers1.png)

![TDM_of_FC_Layers2](./Images/6.3-12-TDM_of_FC_Layers2.png)

- 全连接层映射：具体计算指令的顺序；

![Algorithm_Reflection_of_FC_Layers](./Images/6.3-13-Algorithm_Reflection_of_FC_Layers.png)

- 卷积层映射

![Algorithm_Reflection_of_Conv_Layers](./Images/6.3-14-Algorithm_Reflection_of_Conv_Layers.png)

|                                              图示                                              |                                                                                                                                  描述                                                                                                                                  |
| :---------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| ![Algorithm_Reflection_of_Conv_Layers2](./Images/6.3-15-Algorithm_Reflection_of_Conv_Layers2.png) | $N$个向量MAC计算 $N$个输出神经元，其中这些输出神经元属于 $N$个输出feature map。片内载入数据是所有输入feature map的第一行（ $Nic\times Nif$个输入），而正在计算的这 $N$个输出神经元只和 $Kc\times Nif$个输入相关，所需要的权值是 $Kc\times Nif\times N$个 |
| ![Algorithm_Reflection_of_Conv_Layers3](./Images/6.3-16-Algorithm_Reflection_of_Conv_Layers3.png) |                当计算完成$N$个输出神经元中与 $Kc\times Nif$个输入相关的部分，开始下面 $N$个输出神经元。同样的，这 $N$个神经元属于 $N$个不同的输出feature map。在这个计算过程中，上 $N$个输出神经元的 $Kc\times Nif\times N$个权值可以复用                |
| ![Algorithm_Reflection_of_Conv_Layers4](./Images/6.3-17-Algorithm_Reflection_of_Conv_Layers4.png) |                                                                                         当开始计算不同位置的$N$个神经元时，所依赖的输入与之前输入重叠，而权值则可以完全复用                                                                                         |
| ![Algorithm_Reflection_of_Conv_Layers5](./Images/6.3-18-Algorithm_Reflection_of_Conv_Layers5.png) |                                                                                     重复以上过程，只要计算的输出神经元属于同一个面($Nic\times Nof$)，所依赖的权值就不会发生变化                                                                                     |
| ![Algorithm_Reflection_of_Conv_Layers6](./Images/6.3-19-Algorithm_Reflection_of_Conv_Layers6.png) |                              当与一个面的输入神经元计算都完成（即所有输入feature mape的第一行），后续计算已经不再需要这些输入神经元，片上可重新载入第二个面（即所有输入feature map的第二行），开始进行新的输出神经元的计算。重复以上过程                              |

- 池化层映射；*某种意义上，是对卷积层的简化*

![Algorithm_Reflection_of_Pooling_Layers](./Images/6.3-20-Algorithm_Reflection_of_Pooling_Layers.png)

池化层的计算顺序与卷积层一致，只是计算是 $\max()$或 $\mathrm{avg}()$；

## 6.4 优化设计

### 基于标量MAC的运算部件

将普通标量MAC堆叠成MAC阵列，有几种连接方式

![Scalar_MAC_to_Vector_MAC](./Images/6.4-1-Scalar_MAC_to_Vector_MAC.png)

![Vector_MAC](./Images/6.4-2-Vector_MAC.png)

- 卷积运算中的数据复用

![Reuse_in_Conv](./Images/6.4-3-Reuse_in_Conv.png)

![A_Realization_of_Reusing_PEs](./Images/6.4-4-A_Realization_of_Reusing_PEs.png)

- 对比：向量MAC单元与标量MAC单元

|                    |      基于向量MAC计算单元      |           基于标量MAC计算单元           |
| :----------------: | :----------------------------: | :--------------------------------------: |
|        大小        |        16个16维向量MAC        |           256个MAC，16×16阵列           |
|     乘法器个数     |              256              |                   256                   |
|     加法器个数     |          240(16×15)          |                   256                   |
| 每拍需求外部操作数 |              512              |                    32                    |
|      操作粒度      |           向量、矩阵           |                向量、矩阵                |
|     卷积层映射     | 输入神经元复用、输出神经元复用 | 输入神经元复用、输出神经元复用、权重复用 |
|        优点        | 高效支持矩阵向量映射，灵活性高 |   专用数据流高效支持卷积，带宽需求降低   |
|        缺点        |  依赖外部数据排布，带宽需求高  |    灵活性差，支持其他算子其他特性困难    |

- 非基于MAC运算单元

![Non-MAC](./Images/6.4-5-Non-MAC.png)

### 稀疏化

![Sparse_Synapsis](./Images/6.4-6-Sparse_Synapsis.png)

```mermaid
flowchart LR
	原始网络 --> 网络训练 --> 粗粒度减枝与重训练 --> 局部量化 --> 熵编码 --> 压缩网络
```

### 低位宽

![Low_Bit_Width](./Images/6.4-7-Low_Bit_Width.png)

### 性能评价

- TOPS(Tera Operations Per Second)

$$
TOPS = f_c(\mathrm{GHz})\times (N_{mul}+N_{add})/1000
$$

- 访存带宽：主频 $f_m$、存储位宽 $b$、访存效率 $\eta$

$$
BW=f_m\times b\times\eta
$$

- 基准测试程序：MLPerf

![MLPerf_Baselines](./Images/6.4-8-MLPerf_Baselines.png)

### 影响性能的因素

$$
T=\sum_i N_i\times C_i/f_c
$$

- $N_i$表示该任务中第$i$类操作的数量；
- $C_i$表示完成第$i$类操作需要的时钟周期数；
- $f_c$处理器主频；

优化：

- 减小 $C_i$；
- 减少访存开销；
- 多级并行；

## 6.5 其他加速器

- GPU
  - 计算：SIMD(SIMT)
  - 存储：多层次
  - 控制：SIMD指令
- FPGA
  - 计算：可配置CLB
  - 存储Block RAM
  - 控制：配置

| 类别 |       目标       | 速度 | 能效 |      灵活性      |
| :--: | :--------------: | :--: | :--: | :--------------: |
| DLP |   深度学习专用   |  高  |  高  | 深度学习领域通用 |
| FPGA | 通用的可编程电路 |  低  |  中  |       通用       |
| GPU | SIMD架构矩阵加速 |  中  |  低  |  矩阵类应用通用  |
