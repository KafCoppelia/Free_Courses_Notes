# Chapter 6 深度学习处理器原理
Revision：1

## 6.1 深度学习处理器概述
### 研究意义
大而深的网络
![Deep_Network](./Images/6.1-1-Deep_Network.png)
为什么需要深度学习处理器？
- 深度学习应用广泛：
	- 图像识别、语音处理、自然语言处理、博弈游戏等领域；
	- 已渗透到云服务器和智能手机的方方面面；
- 通用CPU/GPU处理人工神经网络效率低下：
	- 谷歌大脑：1.6万个CPU核跑数天完成猫脸识别训练；
	- AlphaGo：下棋用了1202个CPU和200个GPU；

- 图像处理->GPU
- 信号处理->DSP
- 智能处理->?
- 未来每台计算机可能都需要一个专门的深度学习处理器：
	- 从云服务器到智能手机；
	- 应用面将超过GPU：每年**数十亿**片；

### 发展历史
- 第一次热潮（1950年代～1960年代）
	- 1951，M. Minsky研制了神经网络模拟器SNARC；
	- 1960，F. Rosenblatt研制了神经网络计算器Mark-I；深度学习处理器发展的三个因素：
![First_Climax](./Images/6.1-2-First_Climax.png)
- 第二次热潮（1980年代～1990年代初）
	- 1989，Intel ETANN；
	- 1990，CNAPS；
	- 1993，MANTRA I；
	- 1997，预言神；
	- ……
- 第三次热潮（2006～至今）

1990s的神经网络处理器
- 结构简单
- 规模小
![ETANN](./Images/6.1-3-ETANN.png)
![Other_DLPs](./Images/6.1-4-Other_DLPs.png)
![Third_Climax](./Images/6.1-5-Third_Climax.png)

深度处理器发展的三个因素：
1. 架构
2. 技术
3. 应用

### 设计思路
深度学习处理器的定位：
- 通用性稍低于GPU，能效高于GPU；
- 通用性比ASICs高，但能效稍低于ASICs；
![Position_of_DLP](./Images/6.1-6-Position_of_DLP.png)

如何设计一个深度学习处理器DLP：
- 目标？
- 体系结构？
- 微体系结构？
- 可编程性？

设计思路：
- 整个体系结构最重要的问题：
	- 算法范围界定；
	- 算法分析（计算特性、访存特性）；
- 谁是我们的“朋友”？
	- 自定制硬件，可利用算法特性；
	- 高效率；
- 谁是我们的“敌人”？
	- 阻碍高效率：带宽、访存速度、访存代价；

## 6.2 目标算法分析
*VGG19为例*
![Structure_of_VGG19](./Images/6.2-7-Structure_of_VGG19.png)
### 分析什么？
体系结构设计人员应分析什么？
- 计算
	- 是否存在固定重复的计算模式；
- 访存
	- 数据的局部性；
	- 数据和计算的关系（对于带宽的需求）；

### 全连接层
![Layer_of_FC](./Images/6.2-8-Layer_of_FC.png)
$$\boldsymbol{y}[j]=G\left(\boldsymbol{b}[j]+\displaystyle\sum_{i=0}^{N_i-1}\boldsymbol{W}[j][i]\times\boldsymbol{x}[i]\right)$$
- 代码实现：
```cpp
// x是输入神经元，y是输出神经元，W是权重
y(all) = 0; // 初始化所有输出神经元
	for (j=0; i<No; i++) //外循环复用，复用距离等于Ni
	{
		for (i=0; i<Ni; i++) //内循环复用，距离=1
		{
			y[j] += W[j][i]*x[i]; //内外循环无复用
			if (i==Ni)
			  	y[j] = G(y[j]+b[j]);
		}
  	}
}
```
- 计算特点
	- 向量内积、向量的元素操作；
	- 无复杂控制流；

### 卷积层
- VGG19中第一层
![First_Conv_Layer_of_VGG19](./Images/6.2-9-First_Conv_Layer_of_VGG19.png)
$$\boldsymbol{y}[nor][moc][j]=G\left(\boldsymbol{b}[j]+\displaystyle\sum_{i=0}^{N_{if}-1}\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}\boldsymbol{W}[k_r][k_c][j][i]\times\boldsymbol{X}[r+k_r][c+k_c][i]\right)$$
- 代码实现：
  ```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (j=0; j<Nof; j++)
			sum[j] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (j=0; j<Nof; j++)
					for (i=0l; i<Nif; i++)
						sum[j] += W[kr][kc][j][i]*X[r+kr][c+kc][i];
		for (j=0; j<Nof; j++)
			Y[nor][noc][j] = G(sum[j]+b[j]);
		noc++;
	}
	nor++;
}
  ```
- 计算特点
	- 矩阵内积、向量的元素操作；
	- 无复杂控制流；

### 池化层
![Pooling_Layers](6.2-10-Pooling_Layers.png)
- Max Pooling：
$$\boldsymbol{Y}[nor][noc][i]=\max_{0\le k_c\le K_c,0\le k_r\le K_r}(\boldsymbol{X}[r+kr][c+kc][i])$$
- AVG Pooling：
$$\boldsymbol{Y}[nor][noc][i]=\dfrac{1}{K_c\times K_r}\displaystyle\sum_{k_c=0}^{K_c-1}\sum_{k_r=0}^{K_r-1}(\boldsymbol{X}[r+kr][c+kc][i])$$
- 代码实现：
  ```cpp
nor = 0;
for (r=0; r<Nir; r+=sr) { // sr是垂直方向的池化步长
	noc = 0;
	for (c=0; c<Nic; c+=sc) { // sc是水平方向的池化步长
		for (i=0; i<Nif; i++)
			value[i] = 0;
		for (kr=0; kr<Kr; kr++)
			for (kc=0; kc<Kc; kc++)
				for (i=0; i<Nif; i++) {
					// for average pooling
					value[i] += X[r+kr][c+kc][i];
					// for max pooling
					value[i] +=max(value[i], X[r+kr][c+kc][i]);
				}
		for (i=0; i<Nif; i++) {
			// for average pooling
			Y[nor][noc][i] = value[i]/Kr/Kc;
			// for max pooling
			Y[nor][noc][i] = value[i];
		}
		noc++;
	}
	nor++;
}
  ```
- 计算特点：
	- 向量的元素操作；
	- 无复杂控制流；

### CNN计算特征
不同层的计算特点：
| 层 | 计算类型 | 乘加操作个数 | 激活函数操作个数 |
| :------: | :--------------------------------------: | :-------------------------------: | :-------------------------------: |
|  卷积层 |  矩阵内积、向量的元素操作 | $N_{if}\times N_{of}\times N_{or}\times N_{oc}\times K_r\times K_c$个乘加 | $N_{of}\times N_{or}\times N_{oc}$ |
|  池化层 | 当向量的元素操作 | $N_{if}\times N_{or}\times N_{oc}\times {K_r}\times K_c$个加法或比较+$N_{if}\times N_{or}\times N_{oc}$个除法操作（平均池化） | 无 |
| 全连接层 | 矩阵乘向量、向量的元素操作 | $N_o\times N_i$个乘加 | $N_o$ |

访存特征：
- 全连接层：可解耦性、可复用性；
![全连接层访存特性](6.2-11-全连接层访存特性.png)
- 权值数据量大无复用，带宽需求高，需进行循环分块（可对卷积层、池化层）；
![全连接层访存特性2](./Images/6.2-12-全连接层访存特性2.png)
![Tiling](./Images/6.2-13-Tiling.png)
- 全连接层做两层循环分块(tiling)
```c
for (jjj=0; jjj<No; jjj+=Tjj) { // 对输出神经元进行分块，Tjj和Tj是两层分块大小
	for (jj=jjj; jj<jjj+Tjj; jj+=Tj) {
		for (j=jj; j<jj+Tj; j++)
			y[j] = 0;
		for (iii=0; iii<Ni; iii+=Tii) { // 对输入神经元进行分块，Tii和Ti是两层分块大小
			for (ii=iii; ii<iii+Tii;ii+=Ti)
				for (j=jj; j<jj+Tj; j++)
					for (i=ii; i<ii+Ti; i++)
						sum[j] += W[j][i]*x[i];
		}
		for (j=jj; j<jj+Tj; j++)
			y[j] = G(sum[j] + b[j]);
	}
}
```
![Tiling_of_FC](6.2-14-Tiling_of_FC.png)

不同层的重用特性：
|  层  |  可重用  |  不可重用  |
|  :----------:  |  :------------:  |  :-----------:  |
|  卷积层  |  输入神经元、输出神经元、突触权重   |  无   |
|  池化层  | 当池化窗口大于步长，部分输入神经元可重用 | 池化窗口小于等于步长时，输入神经元、输出神经元均不可重用  |
|  全连接层  |   输入神经元、输出神经元  |   突触权重  |

## 6.3 深度学习处理器DLP结构

DLP结构

- 指令集
- 流水线
- 运算部件
- 访存部件
- 算法映射

#### 指令集

- 计算机的抽象模型
  - 定义了体系结构；
  - 软硬件的唯一接口；
- 为什么采用指令集？
  - 灵活性：支持未来可能出现的新的深度学习算法；
  - 通用性：支持广泛的深度学习算法；

- 设计原则
  - Data-level Parallelism；
  - 可向量化操作；
  - Load-store结构：只通过load和store指令访问主存；
  - 64位定长指令，变长操作数（寄存器指定长度）；
- 控制指令、数据传输指令（Load/Store、MOVE）、计算指令（矩阵运算、向量运算、标量运算）、逻辑指令（向量逻辑、标量逻辑）

#### 流水线

- 7段流水：取值、译码、发射、读寄存器、执行、写回、提交

  ![image-20220317224438674](C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20220317224438674.png)

- MAC(Multiply-Accumulator)：标量MAC单元与向量MAC单元

- 运算部件
  - N个向量MAC单元堆叠；
  - 能够支撑DLP指令集；
  - 激活函数处理单元：非线性函数单元；
  - 池化操作：MFU的三个stage的退出通路；
  - 任意规模：局部累加功能；

#### 访存部件

**非常关键**

- 可解耦性
  - 三个分离访存部件：NRAM-in，NRAM-out，WRAM；
  - 有效避免访存流之间互相干扰

- 可复用性
  - 片上缓存：形成“运算单元-片上-片外”的存储架构；
  - Scratchpad Memory管理；
  - 提高片上数据复用率；

#### 算法映射

- 基本思想：硬件**分时复用**
- 全连接层映射：具体计算指令的顺序；
- 卷积层映射；
- 池化层映射；

## 6.4 优化设计与性能评价

 #### 基本运算单元

- 卷积运算中的数据复用

- 稀疏化

  原始网络→网络训练→粗粒度减枝与重训练→局部量化→熵编码→压缩网络

- 低位宽
- 串行乘法器

#### 性能评价

- TOPS(Tera Operations Per Second)

  $TOPS = f_c(\mathrm{GHz})\times (N_{mul}+N_{add})/1000$

- 访存带宽

  $BW=f_m\times b\times\eta$

  $f_m$存储器主频，存储位宽$b$，访存效率$\eta$；

- 基准测试程序

  - MLPerf

#### 影响性能的因素

$T=\sum_i N_i\times C_i/f_c$

$N_i$表示该任务中第$i$类操作的数量；

$C_i$表示完成第$i$类操作需要的时钟周期数；

$f_c$处理器主频；

- 减小$C_i$；
- 减少访存开销；
- 多级并行；

## 6.5 其他加速器

| 类别 |       目标       | 速度 | 能效 |      灵活性      |
| :--: | :--------------: | :--: | :--: | :--------------: |
| DLP  |   深度学习专用   |  高  |  高  | 深度学习领域通用 |
| FPGA | 通用的可编程电路 |  低  |  中  |       通用       |
| GPU  | SIMD架构矩阵加速 |  中  |  低  |  矩阵类应用通用  |
